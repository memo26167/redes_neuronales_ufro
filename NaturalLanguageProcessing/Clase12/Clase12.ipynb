{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 12\n",
    "Se utilizara la libreria nltk y se aprenderan distintos tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chile', 'ganó', 'la', 'Copa', 'América', 'el', 'año', '2015', 'y', 'el', 'año', '2016']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Chile ganó la Copa América el año 2015 y el año 2016'\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeBankWordTokenizer\n",
    "Es un tokenizador para el idioma inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'in',\n",
       " 'all',\n",
       " ',',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'just',\n",
       " 'another',\n",
       " 'brick',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wall',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"All in all, you're just another brick in the wall.\"\"\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual tokenize\n",
    "Tokenizador para redes sociales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estuvo', 'muuuuuuy', 'bueno', 'el', 'concierto', '!', '!', '!', 'Grandeeee', '@PinkFloyd', ':', '\"', ')', '\"']\n",
      "['Estuvo', 'muuuy', 'bueno', 'el', 'concierto', '!', '!', '!', 'Grandeee', ':', '\"', ')', '\"']\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Estuvo muuuuuuy bueno el concierto!!!!! Grandeeee @PinkFloyd :\")\" \"\"\"\n",
    "\n",
    "print(casual_tokenize(message))\n",
    "\n",
    "print(casual_tokenize(message, reduce_len=True, strip_handles=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "message = \"\"\"Estuvo muuuuuuy bueno el concierto!!!!! Grandeeee @PinkFloyd pic.twitter.com:\")\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estuvo', 'muuuy', 'bueno', 'el', 'concierto', '!', '!', '!', 'Grandeee', 'pic.twitter.com', ':', '\"', ')', '\"']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "print(tweet_tokenizer.tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas\n",
    "Otra forma de analizar texto es utilizando N-gramas,\n",
    "N-grama se define como una secuencia que contiene hasta N elementos que se han extraido desde otra secuencia.\n",
    "\n",
    "Un N-grama de palabras es una secuencia de N palabras dentro de una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Led', 'Zeppelin', 'fue', 'un', 'grupo', 'británico', 'de', 'hard', 'rock', 'fundado', 'en', '1968']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Led Zeppelin fue un grupo británico de hard rock, fundado en 1968.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "\n",
    "tokens = [x for x in tokens if x and x  not in '- \\t\\n.,;!?']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Led', 'Zeppelin'), ('Zeppelin', 'fue'), ('fue', 'un'), ('un', 'grupo'), ('grupo', 'británico'), ('británico', 'de'), ('de', 'hard'), ('hard', 'rock'), ('rock', 'fundado'), ('fundado', 'en'), ('en', '1968')]\n"
     ]
    }
   ],
   "source": [
    "#Bigramas\n",
    "X = list(ngrams(tokens, 2))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Led', 'Zeppelin', 'fue'), ('Zeppelin', 'fue', 'un'), ('fue', 'un', 'grupo'), ('un', 'grupo', 'británico'), ('grupo', 'británico', 'de'), ('británico', 'de', 'hard'), ('de', 'hard', 'rock'), ('hard', 'rock', 'fundado'), ('rock', 'fundado', 'en'), ('fundado', 'en', '1968')]\n"
     ]
    }
   ],
   "source": [
    "#Trigramas\n",
    "X = list(ngrams(tokens, 3))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Led Zeppelin', 'Zeppelin fue', 'fue un', 'un grupo', 'grupo británico', 'británico de', 'de hard', 'hard rock', 'rock fundado', 'fundado en', 'en 1968']\n"
     ]
    }
   ],
   "source": [
    "#Juntar bigramas\n",
    "two_grams = list(ngrams(tokens, 2))\n",
    "Y = [\" \".join(x) for x in two_grams]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque intenta preservar algo de orden, aunque no completo.\n",
    "Tiene el problema de que la primera y ultima palabra solo aparecerán una ves.\n",
    "Tampoco es bueno para la clasificacion, sin embargo si es bueno para la recuperacion de informacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "Las stop words son palabras de conexion, no llevan un significado real, por lo que es una buena practica eliminarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'fire']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'am', 'the', 'on', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La libreria nltk posee una coleccion de stopwords para cada idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\Users\\Memo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con']\n"
     ]
    }
   ],
   "source": [
    "# En el idioma español\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "print(len(stop_words))\n",
    "print(stop_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librearia sklearn tambien posee stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(len(sklearn_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion\n",
    "Convertir todas las letras a minusculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ciencia', 'ciencia', 'ciencia']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['CIENCIA', 'Ciencia', 'CiEnCiA']\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Convierte caracteristicas o conjugaciones de una frase a su estado normal, como plural a singular, genero femenino a masculino y pertenecias(ingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casa\n",
      "mi amigo viven en padre la casa\n"
     ]
    }
   ],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n",
    "print(stem('casas'))\n",
    "print(stem(\"Mis amigos viven en Padre Las Casas\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La libreria nltk posee \"PorterStemmer\", que convierte la \"pertenecia\" (del ingles) y la pluralidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien posee \"SnowballStemmer\" que convierte el genero de las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entre mis amig ella es mi mejor amig'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"Entre mis amigos ella es mi mejor amiga\".split()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
