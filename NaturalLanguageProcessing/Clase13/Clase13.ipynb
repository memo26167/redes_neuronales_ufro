{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 13\n",
    "Se vera otro metodo de vectorizacion, los vectores TF (o frecuencias de terminos), tambien se verá la bolsa de palabras ya que los vectores TF se construiran desde la bolsa de palabras y se medira similitud con estos vectores TF, este enfoque se basara en la frecuencia de las palabras, por lo que es un enfoque estadistico.\n",
    "Si una palabra aparece muchas veces en un documento, podemos asumir que el documento trata sobre ese concepto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['la',\n",
       " 'ufro',\n",
       " 'está',\n",
       " 'en',\n",
       " 'temuco',\n",
       " ',',\n",
       " 'y',\n",
       " 'yo',\n",
       " 'estudio',\n",
       " 'en',\n",
       " 'la',\n",
       " 'ufro',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"La UFRO está en Temuco, y yo estudio en la ufro.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# reducimos el numero de tokens llevando las palbras a minusculas\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crearemos la bolsa de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'la': 2, 'ufro': 2, 'en': 2, 'está': 1, 'temuco': 1, ',': 1, 'y': 1, 'yo': 1, 'estudio': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos ordenar los tokens por frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la', 2),\n",
       " ('ufro', 2),\n",
       " ('en', 2),\n",
       " ('está', 1),\n",
       " ('temuco', 1),\n",
       " (',', 1),\n",
       " ('y', 1),\n",
       " ('yo', 1),\n",
       " ('estudio', 1),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular cuantas veces aparece la palabra 'ufro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "times_ufro_appears = bag_of_words['ufro'] \n",
    "print(times_ufro_appears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular la frecuencia de terminos, debemos dividir por el total tokens unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(bag_of_words)\n",
    "print(num_unique_words)\n",
    "\n",
    "tf = times_ufro_appears/num_unique_words\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dar un ejemplo con mas texto utilizaremos kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "d:\\program files\\python37\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\program files\\python37\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite's wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite.\n",
      "\n",
      "The lift that sustains the kite in flight is generated when air flows around the kite's surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle).\n",
      "\n",
      "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
      "\n",
      "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\n",
      "\n",
      "Kites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text\n",
    "print(kite_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la bolsa de palabras de la misma forma que antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tienen:  180  tokens\n",
      "Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, 'and': 10, 'of': 10, 'kites': 8, 'is': 7, 'in': 7, 'or': 6, 'wing': 5, 'to': 5, 'be': 5, 'as': 5, 'lift': 4, 'have': 4, 'may': 4, 'at': 3, 'so': 3, 'can': 3, 'also': 3, 'kiting': 3, 'are': 3, 'flown': 3, 'tethered': 2, 'craft': 2, 'with': 2, 'that': 2, 'air': 2, 'consists': 2, 'tethers': 2, 'anchors.': 2, 'often': 2, 'bridle': 2, 'wind': 2, \"'s\": 2, 'designed': 2, ';': 2, 'when': 2, 'for': 2, 'moving': 2, 'technical': 2, 'even': 2, 'called': 2, 'surface': 2, 'pressure': 2, 'drag': 2, 'force': 2, 'by': 2, 'which': 2, '.': 2, 'used': 2, 'power': 2, 'traditionally': 1, 'heavier-than-air': 1, 'surfaces': 1, 'react': 1, 'against': 1, 'create': 1, 'drag.': 1, 'wings': 1, 'guide': 1, 'face': 1, 'correct': 1, 'angle': 1, 'it.': 1, 'not': 1, 'needed': 1, 'sailplane': 1, 'launch': 1, 'tether': 1, 'meets': 1, 'single': 1, 'point.': 1, 'fixed': 1, 'untraditionally': 1, 'tether-set-coupled': 1, 'sets': 1, 'though': 1, 'system': 1, 'still': 1, 'kite.': 1, 'sustains': 1, 'flight': 1, 'generated': 1, 'flows': 1, 'around': 1, 'producing': 1, 'low': 1, 'above': 1, 'high': 1, 'below': 1, 'wings.': 1, 'interaction': 1, 'generates': 1, 'horizontal': 1, 'along': 1, 'direction': 1, 'wind.': 1, 'resultant': 1, 'vector': 1, 'from': 1, 'components': 1, 'opposed': 1, 'tension': 1, 'one': 1, 'more': 1, 'lines': 1, 'attached.': 1, 'anchor': 1, 'point': 1, 'line': 1, 'static': 1, '(': 1, 'e.g.': 1, 'towing': 1, 'running': 1, 'person': 1, 'boat': 1, 'free-falling': 1, 'anchors': 1, 'paragliders': 1, 'fugitive': 1, 'parakites': 1, 'vehicle': 1, ')': 1, 'same': 1, 'principles': 1, 'fluid': 1, 'flow': 1, 'apply': 1, 'liquids': 1, 'under': 1, 'water.': 1, 'hybrid': 1, 'comprising': 1, 'both': 1, 'lighter-than-air': 1, 'balloon': 1, 'well': 1, 'lifting': 1, 'kytoon.': 1, 'long': 1, 'varied': 1, 'history': 1, 'many': 1, 'different': 1, 'types': 1, 'individually': 1, 'festivals': 1, 'worldwide.': 1, 'recreation': 1, 'art': 1, 'other': 1, 'practical': 1, 'uses.': 1, 'sport': 1, 'aerial': 1, 'ballet': 1, 'sometimes': 1, 'part': 1, 'competition.': 1, 'multi-line': 1, 'steerable': 1, 'generate': 1, 'large': 1, 'forces': 1, 'activities': 1, 'such': 1, 'surfing': 1, 'landboarding': 1, 'fishing': 1, 'buggying': 1, 'new': 1, 'trend': 1, 'snow': 1, 'kiting.': 1, 'man-lifting': 1, 'been': 1, 'made': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"Se tienen: \",len(token_counts), \" tokens\")\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo podemos ver que \"the\" y \"a\" son las palabras mas frecuentes, estas son \"stop_words\", que debemos eliminar.\n",
    "Tambien nos encontramos con signos de puntuacion como comas, parentesis, etc. Que sería bueno eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kite', 16),\n",
       " ('kites', 8),\n",
       " ('wing', 5),\n",
       " ('lift', 4),\n",
       " ('may', 4),\n",
       " ('also', 3),\n",
       " ('kiting', 3),\n",
       " ('flown', 3),\n",
       " ('tethered', 2),\n",
       " ('craft', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "\n",
    "puntuacion = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"'\", '(', ')', '[', ']'))\n",
    "tokens = [x for x in tokens if x not in puntuacion]\n",
    "\n",
    "kite_counts = Counter(tokens)\n",
    "#Veremos los 10 tokens más comunes\n",
    "kite_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "# ¿Cuantos tokens hay?\n",
    "print(len(kite_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Como vectorizaremos esto? \n",
    "Llenaremos una lista con los valores que tiene el diccionario dividido con el numero total de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07960199004975124, 0.03980099502487562, 0.024875621890547265, 0.01990049751243781, 0.01990049751243781, 0.014925373134328358, 0.014925373134328358, 0.014925373134328358, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453, 0.004975124378109453]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / len(tokens))\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma se ordena el diccionario con los tokens desde el mas frecuente, al menos frecuente.\n",
    "Asi llevamos un lexico y una representacion vectorial con componentes numericas, continuas y entre $0$ y $1$.\n",
    "Ahora crearemos un corpus con 3 documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La UFRO está en Temuco, y yo estudio en la ufro.', 'La Ufro es una universidad estatal.', 'Facultad de Ingeniería y Ciencia, Ufro.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "docs = [\"La UFRO está en Temuco, y yo estudio en la ufro.\"]\n",
    "docs.append(\"La Ufro es una universidad estatal.\")\n",
    "docs.append(\"Facultad de Ingeniería y Ciencia, Ufro.\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construira un lexico con las palabras de los 3 documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[',', '.', 'en', 'en', 'estudio', 'está', 'la', 'la', 'temuco', 'ufro', 'ufro', 'y', 'yo'], ['.', 'es', 'estatal', 'la', 'ufro', 'una', 'universidad'], [',', '.', 'ciencia', 'de', 'facultad', 'ingeniería', 'ufro', 'y']]\n"
     ]
    }
   ],
   "source": [
    "#Corpus completo tokenizado\n",
    "print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para unir todos los tokens del corpus, simplemente los unimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'en', 'en', 'estudio', 'está', 'la', 'la', 'temuco', 'ufro', 'ufro', 'y', 'yo', '.', 'es', 'estatal', 'la', 'ufro', 'una', 'universidad', ',', '.', 'ciencia', 'de', 'facultad', 'ingeniería', 'ufro', 'y']\n",
      "En total hay 28 tokens (no es nuestro lexico)\n"
     ]
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "print(all_doc_tokens)\n",
    "print(f'En total hay {len(all_doc_tokens)} tokens (no es nuestro lexico)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estos tokens le aplicaremos la funcion 'set()' para obtener los tokens unicos que compondran nuestro lexico y luego ordenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[',', '.', 'ciencia', 'de', 'en', 'es', 'estatal', 'estudio', 'está', 'facultad', 'ingeniería', 'la', 'temuco', 'ufro', 'una', 'universidad', 'y', 'yo']\n"
     ]
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print(len(lexicon))\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un vector con todos los tokens y un 0, luego ese 0 lo llenaremos con la frecuencia del token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('ciencia', 0),\n",
       "             ('de', 0),\n",
       "             ('en', 0),\n",
       "             ('es', 0),\n",
       "             ('estatal', 0),\n",
       "             ('estudio', 0),\n",
       "             ('está', 0),\n",
       "             ('facultad', 0),\n",
       "             ('ingeniería', 0),\n",
       "             ('la', 0),\n",
       "             ('temuco', 0),\n",
       "             ('ufro', 0),\n",
       "             ('una', 0),\n",
       "             ('universidad', 0),\n",
       "             ('y', 0),\n",
       "             ('yo', 0)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facultad', 'de', 'ingeniería', 'y', 'ciencia', ',', 'ufro', '.']\n",
      "Counter({'facultad': 1, 'de': 1, 'ingeniería': 1, 'y': 1, 'ciencia': 1, ',': 1, 'ufro': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "doc_vectors = []\n",
    "\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "print(tokens)\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"doc_vectors\" es un vector que posee diccionarios de cada cada documento, en donde está el token y su frecuencia de aparicion en el documento.\n",
    "\n",
    "Ahora convertimos este diccionario a un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for vec in doc_vectors:\n",
    "    vectors.append([val for val in vec.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(',', 0.05555555555555555), ('.', 0.05555555555555555), ('ciencia', 0), ('de', 0), ('en', 0.1111111111111111), ('es', 0), ('estatal', 0), ('estudio', 0.05555555555555555), ('está', 0.05555555555555555), ('facultad', 0), ('ingeniería', 0), ('la', 0.1111111111111111), ('temuco', 0.05555555555555555), ('ufro', 0.1111111111111111), ('una', 0), ('universidad', 0), ('y', 0.05555555555555555), ('yo', 0.05555555555555555)])\n",
      "\n",
      "\n",
      "[0.05555555555555555, 0.05555555555555555, 0, 0, 0.1111111111111111, 0, 0, 0.05555555555555555, 0.05555555555555555, 0, 0, 0.1111111111111111, 0.05555555555555555, 0.1111111111111111, 0, 0, 0.05555555555555555, 0.05555555555555555]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectors[0])\n",
    "print('\\n')\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Como medimos similitud?\n",
    "Podemos ocupar el producto punto?\n",
    "Hicimos una transformacion que nos llevo de $Z^n$ a $R^n$\n",
    "$A \\cdot B = |A||B|cos(\\theta)$\n",
    "\n",
    "entonces\n",
    "\n",
    "$cos(\\theta)=  \\frac{A \\cdot B} {|A||B|}$\n",
    "\n",
    "en donde esta fraccion representa si ocupamos las mismas palabras o no.\n",
    "\n",
    "Si el valor de este $cos(\\theta)$ es cercano a $1$, los vectores usarán palabras similares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
