{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 14\n",
    "## Vectorizacion TF-IDF\n",
    "### Vectores TF\n",
    "Sabemos que los vectores $TF$, para el token $t$ dado el corpus $C$ de un documento $D$, con un lexico $L$ es:\n",
    "\n",
    "$TF(t,D)=  \\frac{\\text{Numero de veces que aparece el token $t$ en el documento $D$}} {\\text{Numero de tokens en el documento $D$}}$\n",
    "### Vectores TF-IDF\n",
    "Calculamos el vector $TF$ pero env es de un corpus, solo de un documento $D$ y de un lexico del documento $L_D$.\n",
    "\n",
    "$$TF(t,D)=  \\dfrac{\\text{Numero de veces que aparece el token $t$ en el documento $D$}} {\\text{Numero de tokens en el documento $D$}}$$\n",
    "\n",
    "Y la parte $IDF$ es\n",
    "\n",
    "$$IDF(t,C)=  \\dfrac{\\text{Numero de documentos en el corpus $C$}} {\\text{Numero de documentos en el corpus $C$ que  contienen $t$}}$$\n",
    "\n",
    "Entonces así \n",
    "\n",
    "$$\\text{TF-IDF}(t,D,C) = \\text{TF}(t,d) \\times ln(\\text{IDF}(t,C))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in d:\\program files\\python37\\lib\\site-packages (0.23.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'd:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in d:\\program files\\python37\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in d:\\program files\\python37\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in d:\\program files\\python37\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in d:\\program files\\python37\\lib\\site-packages (from scikit-learn) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\program files\\python37\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "d:\\program files\\python37\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\program files\\python37\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to d:\\program files\\python37\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\program files\\\\python37\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. a kite consists of wings, tethers, and anchors. kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. a kite's wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. a kite may have fixed or moving anchors. untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite.\n",
      "\n",
      "the lift that sustains the kite in flight is generated when air flows around the kite's surface, producing low pressure above and high pressure below the wings. the interaction with the wind also generates horizontal drag along the direction of the wind. the resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. the anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle).\n",
      "\n",
      "the same principles of fluid flow apply in liquids and kites are also used under water.\n",
      "\n",
      "a hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\n",
      "\n",
      "kites have a long and varied history and many different types are flown individually and at festivals worldwide. kites may be flown for recreation, art or other practical uses. sport kites can be flown in aerial ballet, sometimes as part of a competition. power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. even man-lifting kites have been made.\n",
      "\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "print(kite_intro)\n",
    "intro_total = len(intro_tokens)\n",
    "print(intro_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites were invented in china, where materials ideal for kite building were readily available: silk fabric for sail material; fine, high-tensile-strength silk for flying line; and resilient bamboo for a strong, lightweight framework.\n",
      "\n",
      "the kite has been claimed as the invention of the 5th-century bc chinese philosophers mozi (also mo di) and lu ban (also gongshu ban). by 549 ad paper kites were certainly being flown, as it was recorded that in that year a paper kite was used as a message for a rescue mission. ancient and medieval chinese sources describe kites being used for measuring distances, testing the wind, lifting men, signaling, and communication for military operations. the earliest known chinese kites were flat (not bowed) and often rectangular. later, tailless kites incorporated a stabilizing bowline. kites were decorated with mythological motifs and legendary figures; some were fitted with strings and whistles to make musical sounds while flying. from china, kites were introduced to cambodia, thailand, india, japan, korea and the western world.\n",
      "\n",
      "after its introduction into india, the kite further evolved into the fighter kite, known as the patang in india, where thousands are flown every year on festivals such as makar sankranti.\n",
      "\n",
      "kites were known throughout polynesia, as far as new zealand, with the assumption being that the knowledge diffused from china along with the people. anthropomorphic kites made from cloth and wood were used in religious ceremonies to send prayers to the gods. polynesian kite traditions are used by anthropologists get an idea of early \"primitive\" asian traditions that are believed to have at one time existed in asia.\n",
      "\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "print(kite_history)\n",
    "history_total = len(history_tokens)\n",
    "print(history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo TF-IDF\n",
    "Calcularemos TF-IDF a 3 palabras contenidas en estos textos.\n",
    "Por ejemplo, a la palabra \"kite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Creamos estos diccionarios para guardar valores para ambos documentos\n",
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kite': 0.0440771349862259}\n",
      "{'kite': 0.020202020202020204}\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf)\n",
    "print(history_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que la palabra kite es mas importante en el intro_tf que en el history_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kite': 0.0440771349862259, 'and': 0.027548209366391185}\n",
      "{'kite': 0.020202020202020204, 'and': 0.030303030303030304}\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "print(intro_tf)\n",
    "print(history_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo tambien vemos que la palabra and es mas importante que kite en el history que en el intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calcularemos en cuantos documentos de los dos, encontramos la palabra 'and', 'kite' y 'china'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 2\n",
      "kite 2\n",
      "china 1\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "\n",
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "    \n",
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "    \n",
    "print(f'and {num_docs_containing_and}')\n",
    "print(f'kite {num_docs_containing_kite}')\n",
    "print(f'china {num_docs_containing_china}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calcularemos los IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_idf['and'] = num_docs / (num_docs_containing_and + 1)\n",
    "history_idf['and'] = num_docs / (num_docs_containing_and + 1)\n",
    "\n",
    "intro_idf['kite'] = num_docs / (num_docs_containing_kite + 1)\n",
    "history_idf['kite'] = num_docs / (num_docs_containing_kite + 1)\n",
    "\n",
    "intro_idf['china'] = num_docs / (num_docs_containing_china + 1)\n",
    "history_idf['china'] = num_docs / (num_docs_containing_china + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0.6666666666666666, 'kite': 0.6666666666666666, 'china': 1.0}\n",
      "{'and': 0.6666666666666666, 'kite': 0.6666666666666666, 'china': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(intro_idf)\n",
    "print(history_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calcularemos TF-IDF, sin logaritmo natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and']*intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite']*intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china']*intro_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and']*history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite']*history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china']*history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0.018365472910927456, 'kite': 0.02938475665748393, 'china': 0.0}\n",
      "{'and': 0.0202020202020202, 'kite': 0.01346801346801347, 'china': 0.010101010101010102}\n"
     ]
    }
   ],
   "source": [
    "print(intro_tfidf)\n",
    "print(history_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando con logaritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "intro_tfidf['and'] = intro_tf['and']*math.log(intro_idf['and'])\n",
    "intro_tfidf['kite'] = intro_tf['kite']*math.log(intro_idf['kite'])\n",
    "intro_tfidf['china'] = intro_tf['china']*math.log(intro_idf['china'])\n",
    "\n",
    "history_tfidf['and'] = history_tf['and']*math.log(history_idf['and'])\n",
    "history_tfidf['kite'] = history_tf['kite']*math.log(history_idf['kite'])\n",
    "history_tfidf['china'] = history_tf['china']*math.log(history_idf['china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': -0.011169837688930151, 'kite': -0.01787174030228824, 'china': 0.0}\n",
      "{'and': -0.012286821457823165, 'kite': -0.008191214305215444, 'china': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(intro_tfidf)\n",
    "print(history_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porque se le añade logaritmo natural? por la ley de zipf\n",
    "Si un corpus es suficientemente grande, la palabra más frecuente aparece el doble de veces que la segunda palabra más frecuente (relacion exponencial), entonces añadiendole el logaritmo se elimina esa exponencialidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     D:\\Users\\Memo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus bastante grande!\n",
    "Aqui podemos ver la ley de zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el mismo corpus que la clase pasada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La UFRO está en Temuco, y yo estudio en la ufro.', 'La Ufro es una universidad estatal.', 'Facultad de Ingeniería y Ciencia, Ufro.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "docs = [\"La UFRO está en Temuco, y yo estudio en la ufro.\"]\n",
    "docs.append(\"La Ufro es una universidad estatal.\")\n",
    "docs.append(\"Facultad de Ingeniería y Ciencia, Ufro.\")\n",
    "print(docs)\n",
    "\n",
    "doc_tokens = []\n",
    "tokens_in_doc = []\n",
    "num_tokens_in_doc = []\n",
    "for doc in docs:\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    token_counts = Counter(tokens)\n",
    "    tokens_in_doc.append(tokens)\n",
    "    num_tokens_in_doc.append(len(token_counts))\n",
    "\n",
    "#todos los token de los docs\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "\n",
    "# Léxico\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el diccionario que contiene en cuantos documentos aparece cada token, para calcular el IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents_containing_token = {}\n",
    "for token in lexicon:\n",
    "    containing_token = 0\n",
    "    for i, doc in enumerate(docs):\n",
    "        if token in tokens_in_doc[i]:\n",
    "            containing_token += 1\n",
    "    num_documents_containing_token.update({token : containing_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([(',', 0.10986122886681099), ('.', 0.10986122886681099), ('ciencia', 0), ('de', 0), ('en', 0.21972245773362198), ('es', 0), ('estatal', 0), ('estudio', 0.10986122886681099), ('está', 0.10986122886681099), ('facultad', 0), ('ingeniería', 0), ('la', 0.21972245773362198), ('temuco', 0.10986122886681099), ('ufro', 0.21972245773362198), ('una', 0), ('universidad', 0), ('y', 0.10986122886681099), ('yo', 0.10986122886681099)]), OrderedDict([(',', 0), ('.', 0.15694461266687282), ('ciencia', 0), ('de', 0), ('en', 0), ('es', 0.15694461266687282), ('estatal', 0.15694461266687282), ('estudio', 0), ('está', 0), ('facultad', 0), ('ingeniería', 0), ('la', 0.15694461266687282), ('temuco', 0), ('ufro', 0.15694461266687282), ('una', 0.15694461266687282), ('universidad', 0.15694461266687282), ('y', 0), ('yo', 0)]), OrderedDict([(',', 0.13732653608351372), ('.', 0.13732653608351372), ('ciencia', 0.13732653608351372), ('de', 0.13732653608351372), ('en', 0), ('es', 0), ('estatal', 0), ('estudio', 0), ('está', 0), ('facultad', 0.13732653608351372), ('ingeniería', 0.13732653608351372), ('la', 0), ('temuco', 0), ('ufro', 0.13732653608351372), ('una', 0), ('universidad', 0), ('y', 0.13732653608351372), ('yo', 0)])]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "document_tfidf_vectors = []\n",
    "for i, doc in enumerate(docs):\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        tf = value / num_tokens_in_doc[i]\n",
    "        idf = len(docs) / num_documents_containing_token[token]\n",
    "        vec[key] = tf * math.log(idf)\n",
    "    document_tfidf_vectors.append(vec)\n",
    "    \n",
    "print(document_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([(',', 0.10986122886681099), ('.', 0.10986122886681099), ('ciencia', 0), ('de', 0), ('en', 0.21972245773362198), ('es', 0), ('estatal', 0), ('estudio', 0.10986122886681099), ('está', 0.10986122886681099), ('facultad', 0), ('ingeniería', 0), ('la', 0.21972245773362198), ('temuco', 0.10986122886681099), ('ufro', 0.21972245773362198), ('una', 0), ('universidad', 0), ('y', 0.10986122886681099), ('yo', 0.10986122886681099)]), OrderedDict([(',', 0), ('.', 0.15694461266687282), ('ciencia', 0), ('de', 0), ('en', 0), ('es', 0.15694461266687282), ('estatal', 0.15694461266687282), ('estudio', 0), ('está', 0), ('facultad', 0), ('ingeniería', 0), ('la', 0.15694461266687282), ('temuco', 0), ('ufro', 0.15694461266687282), ('una', 0.15694461266687282), ('universidad', 0.15694461266687282), ('y', 0), ('yo', 0)])]\n"
     ]
    }
   ],
   "source": [
    "print(document_tfidf_vectors[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de la similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sim_coseno(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v*vec2[i]\n",
    "        \n",
    "    norm_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    norm_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    return dot_prod / (norm_1 * norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4335549847620599"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_coseno(document_tfidf_vectors[0], document_tfidf_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43355498, 0.40555355],\n",
       "       [0.43355498, 0.        , 0.26726124],\n",
       "       [0.40555355, 0.26726124, 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "similitud_tfidf = [[sim_coseno(doc1, doc2) for doc2 in document_tfidf_vectors] for doc1 in document_tfidf_vectors]\n",
    "similitud_tfidf = np.array(similitud_tfidf)\n",
    "np.fill_diagonal(similitud_tfidf, 0)\n",
    "similitud_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libreria sklearn\n",
    "Python posee todo esto programado en 4 lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1, encoding='utf-8')\n",
    "model = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.5844829  0.         0.         0.29224145\n",
      "  0.29224145 0.         0.         0.44451431 0.29224145 0.34520502\n",
      "  0.         0.         0.29224145]\n",
      " [0.         0.         0.         0.45050407 0.45050407 0.\n",
      "  0.         0.         0.         0.34261996 0.         0.26607496\n",
      "  0.45050407 0.45050407 0.        ]\n",
      " [0.47952794 0.47952794 0.         0.         0.         0.\n",
      "  0.         0.47952794 0.47952794 0.         0.         0.28321692\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
